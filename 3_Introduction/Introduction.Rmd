# Introduction
Evaluation metrics are essential to identify effective classification models [@Bekkar2013; @Chen2016; @Chicco2020; @Espindola2005; @Ferri2009; @Haixiang2017; @Hossin2015; @Kautz2017; @Li2014; @Lopez2013; @Luque2019; @Maratea2014; @Pereira2020; @Stehman1997; @Sun2007; @Tharwat2021; @Zhu2017]. Choosing the right metric for a specific classification problem can be challenging, as different metrics measure different aspects [@Caruana2004; @Ferri2009; @Hossin2015; @Kautz2017; @Pereira2020] and there is no consensus on which metrics are most suitable [@Branco2017; @Chicco2020; @Delgado2019; @Jurman2012; @Pereira2018; @Pereira2020; @Sokolova2009]. This lack of consensus is problematic because different metrics can lead to conflicting conclusions when comparing models [@Ferri2009; @Kautz2017;  @Racz2019; @Stehman1997; @Zhu2017]. The selection of an adequate performance metric is therefore of great importance. 

## Metric Selection Challenges with Imbalanced Data
Metric selection is even more challenging when dealing with imbalanced data [@Bekkar2013; @Luque2019; @Picek2018]. In these scenarios the observations are not evenly distributed across classes [@Loyola-gonzalez2016a; @Loyola-gonzalez2016b]. Class imbalance can affect both the meaning and values of metrics [@Luque2019], sometimes making them more indicative of the level of imbalance than of actual classification performance [@Jeni2013]. Furthermore, in imbalanced scenarios, metrics can behave differently [@Ballabio2018; @Ferri2009; @Jeni2013; @Loyola-gonzalez2016b; @Luque2019; @Racz2019; @Zhuqiuming2020] and the differences between them are larger [@Ferri2009; @Pereira2020]. Metric selection is therefore not only complex in imbalanced scenarios but also more critical, as choices have greater impact on the interpretation of the result.

Typical for domains characterized by imbalanced data is that classes are not equally important. This is particularly true in fields such as finance (e.g., credit risk assessment), biochemistry (e.g., protein detection), and medicine (e.g., diagnosis). In these fields, predicting the rare examples is often of main interest [@Branco2017; @Chen2016; @Haixiang2016; @Haixiang2017] and misclassifying a minority class instance can have severe consequences [@Daskalaki2006; @Haixiang2016; @Haixiang2017; @Hegarcia2009; @Marques2013; @Saez2016; @Shenfield2017; @Xia2018; @Yang2009; @Zhou2006; @Zhou2010]. Thus, despite their smaller size, minority classes are typically the classes of interest in imbalanced classification tasks [e.g., @Krawczyk2016; @Li2014; @Lopez2013]. The selection of metrics that incorporate this non-uniform importance is therefore essential [@Branco2017; @Daskalaki2006; @Maratea2014; @Mullick2020].

Class imbalance is common and one of the most challenging aspects in classification [@Bekkar2013; @Bizhang2018; @Chen2016; @Fernandes2019; @Tahir2019], even after decades of intensive research [@Krawczyk2016]. Class imbalance complicates the entire classification process, affecting every stage from data pre-processing to model evaluation [@Liu2015; @Peng2014; @Luque2019]. The underlying issue is that majority class(es) have an inherently larger weight in this process because they have a larger number of observations compared to the minority class(es) or underrepresented class(es). This harms feature selection and results in degraded performance especially on minority classes because classifiers are "biased" (i.e., disproportionately oriented) to correctly predicting majority class instances [e.g., @Agrawal2015; @Khan2018; @Loyola-gonzalez2016a; @Lopez2012]. A problem that can persist even with resampling methods [@Krawczyk2016; @Waldner2019; @Wang2012]. Evaluation metrics should highlight the degraded performance, reflect the importance of minority classes [@Branco2017; @Daskalaki2006] and compensate for classification bias towards majority classes [@Hossin2015; @Maratea2014; @Mullick2020]. However, many metrics are themselves biased towards majority class performance and can mask poor performance on minority classes [@Ballabio2018; @Bekkar2013; @Buda2018; @Jeni2013; @Loyola-gonzalez2016a; @Garcia2006; @Garcia2009; @Garcia2012; @Haixiang2016; @Haixiang2017; @Hossin2015; @Krawczyk2014; @Mullick2020; @Tahir2019]. Evaluation metrics that primarily provide insight into majority classes are of limited use in practice [@Bekkar2013; @Branco2017; @Buda2018; @Garcia2006; @Haixiang2016; @Haixiang2017; @Pereira2020]. Moreover, when these metrics are also part of the optimization process, they can amplify the classification issues described before [@Boughorbel2017; @Du2017; @Hossin2015; @Kautz2017; @Krawczyk2014]. Note that this thesis is focused exclusively on the application of metrics for model evaluation and not their role in optimization, even though these aspects are interconnected.

Evaluation metrics that are most widely used evaluate the confusion matrix [@Hezhang2018; @Luque2019], which represents the test result of the classification model by showing the distribution of observations between actual and predicted classes. Commonly used metrics in imbalanced classification tasks [as reviewed by @Haixiang2017, covering research from \mbox{2006-2016}] are: Accuracy [@ZhangBi2019]; F1-score [F1, @Kautz2017; @Li2019a;  @Picek2018; @Sun2018; @Widera2020; @Wong2022; @ZhangBi2019; @Zhang2022] and its counterpart macro-averaged F1 (F1~m~) for multiclass scenarios [@Collell2018; @Kautz2017]; and Matthews correlation coefficient [MCC, @Delgado2019; @Kautz2017; @Picek2018; @Zhuqiuming2020]. It is well-established that Accuracy is unsuitable for imbalanced classification [e.g., @Ballabio2018; @Chicco2020; @Hegarcia2009] because it fails to account for class distribution [@Boughorbel2017; @Delgado2019; @Garcia2009; @Weiss2003], it assumes unequal misclassification costs [@Garcia2009; @Garcia2018; @Maratea2014; @Marques2013; @Shenfield2017; @Weiss2003], and it sometimes even completely ignores the performance on the minority classes [@Hezhang2018]. F1 and F1~m~ have been proposed as improved solutions for imbalanced scenarios either for binary [@Bekkar2013; @Brown2018; @Garcia2009; @Haixiang2016; @Haixiang2017; @Hegarcia2009; @Hezhang2018; @Hossin2015; @Jeni2013; @Kautz2017; @Li2020; @Li2014; @Picek2018;  @Wong2022; @Zarinabad2017] or multiclass tasks [@Collell2018; @Garcia2018; @Haixiang2016; @Tan2005; @Yang1999]. Yet, similar to Accuracy, both metrics have been criticized in imbalanced scenarios for not providing adequate insight into minority class performance [@Alejo2013; @Branco2017; @Chicco2017; @Chicco2020; @Garcia2009; @Lopez2013; @Maratea2014; @Sanchez-Crisostomo2014]. In response to these limitations, MCC has gained popularity [@Gorodkin2004; @Boughorbel2017; @Chicco2017; @Delgado2019] in both balanced and imbalanced scenarios [@Brown2018; @Jurman2012], as a robust [@Boughorbel2017; @Chicco2020; @Delgado2019], reliable, and realistic metric [@Brown2018; @Chicco2020]. MCC is therefore considered a better alternative to F1~(m)~  [@Boughorbel2017; @Brown2018; @Chicco2017; @Chicco2020; @Luque2019; @Pereira2020] as well as a better [@Bekkar2013; @Brown2018; @Delgado2019; @Boughorbel2017; @Jurman2012; @Luque2019; @Pereira2020; @Picek2018], more truthful, and informative metric than Accuracy [@Chicco2017; @Chicco2020]. With a relatively low bias towards either class, MCC is claimed to be the best binary performance metric if both predictive hits and misses are considered important [@Luque2019]. However, in multiclass scenarios MCC is less extensively studied and often not included in comparative analyses [e.g., @Ferri2009; @Mullick2020; @Sokolova2009]. The existing evidence for MCC’s suitability for evaluating multiclass imbalanced classification outcomes is mixed, with some studies indicating its effectiveness [@Delgado2019; @Jurman2012] and others suggesting the opposite [@Ballabio2018; @Branco2017; @Kautz2017; @Picek2018; @Valverde-Albacete2014]. MCC’s inconsistent performance in some multiclass scenarios highlight a broader issue: metrics can act differently in multiclass versus binary scenarios [@Ballabio2018; @Ferri2009; @Racz2019; @Sokolova2009]. 

## Challenges in Multiclass (Imbalanced) Scenarios 
Research has primarily focused on binary classification [@Branco2017; @Hossin2015; @Liao2008; @Phoungphol2012; @Saez2016; @ZhangBi2019], resulting in a growing consensus on generally accepted metrics for binary classification [@Kautz2017]. As these metric solutions are originally developed for binary classification [@Haixiang2016; @Haixiang2017; @Hossin2015], they are not directly applicable to multiclass scenarios [@Haixiang2016; @Haixiang2017; @Hossin2015; @Saez2016; @Shenfield2017]. While they can often be extended [@Branco2017; @Haixiang2017; @Hegarcia2009; @Pereira2017], there is a lack of consensus on how to properly do so [@Espindola2005; @Phoungphol2012]. Some extending approaches even increase the level of imbalance through one-vs-all strategies [@Espindola2005], decrease metrics' sensitivity to minority classes due to averaging [@Sanchez-Crisostomo2014], or are simply inaccurate as a result of information loss [@Gorodkin2004]. Thus, these extended metrics can have different characteristics [@Sokolova2009] and might therefore be of limited use in multiclass scenarios [@Branco2017; @Delgado2019; @Hossin2015; @Kautz2017; @Sanchez-Crisostomo2014]. 

The core issue is that multiclass scenarios are far more complex than binary ones. Data complexities (e.g., class imbalance, class overlap, and within-class sub-clusters) are more severe in multiclass scenarios [@Krawczyk2016; @Garcia2018; @Saez2016; @Fernandez2013; @Wang2012], because they can now occur locally (i.e., between two classes) or globally (i.e., among several classes). Also, new data complexities arise in multiclass scenarios where the class distribution is imbalanced: in these cases there are multiple class compositions (i.e., ratio and number) of minority and majority classes possible, each with a potential different impact that needs to be addressed [@Branco2017; @Saez2016; @Wang2012]; and classes can simultaneously be a minority or majority class depending on the reference class [@Saez2016]. Developing metrics specifically for multiclass imbalanced scenarios is therefore more complex [@Hossin2015]. There are more aspects of classification performance to focus on [@Branco2017] and a major challenge is representing and evaluating performance for all classes simultaneously [@Mullick2020]. While a multi-objective approach (i.e., combining sensitivity and precision) is advised for imbalanced scenarios [@Daskalaki2006; @Hancock2023; @Lopez2013; @Mullick2020], implementing this in multiclass classification is difficult [@Mullick2020]. These difficulties are evidenced by the fact that the differences between existing metrics in multiclass imbalanced scenarios are larger in comparison to binary and balanced ones [@Ferri2009]. The selection of a suitable metric is therefore even more critical in multiclass imbalanced scenarios.

The aforementioned factors complicate not only the evaluation of classification performance but also classification itself [@Bizhang2018; @Garcia2018; @Fernandez2013; @Haixiang2017; @Krawczyk2016; @Lango2018; @Saez2016; @Wang2012]. A widely used approach to work around the data complexities is binarization [@Agrawal2015; @Chang2017; @Du2017; @Fernandes2019; @Fernandez-Navarro2011; @Haixiang2017; @Krawczyk2016; @Lango2018; @Yang2018; @ZhaoLi2008; @ZhangBi2019; @Wang2012]. In binarization the classification task is decomposed into multiple binary problems, thereby allowing the application of binary metrics [@Haixiang2017; @Luque2019; @Liao2008]. However, the effectivity of binarization is questioned for multiclass imbalanced scenarios [@Fernandes2019;  @Garcia2018; @Lango2018; @Li2020; @Wang2012] because it can increase the level of imbalance within subproblems [@Liu2015] and important information might be lost in de process [@Fernandez-Navarro2011; @Li2019a; @Li2020; @Saez2016; @Wang2012]. This leads to models with a degraded performance [@Agrawal2015;  @Krawczyk2016; @Yuan2018] on minority classes and overall accuracy [@Lango2018; @Wang2012]. Also, the performance evaluations themselves are considered inaccurate as a result of the information loss [@Gorodkin2004]. Hence, binarization is not an adequate solution to overcome evaluation challenges in multiclass imbalanced classification.

Despite efforts to develop suitable metrics for binary scenarios, generally accepted solutions for multiclass imbalance scenarios are still lacking [@Kautz2017]. @Branco2017 demonstrated that no single commonly used metric (including Accuracy, F1~m~, and MCC) performed well in all multiclass imbalance scenarios (i.e., class compositions with: multi-minority, multi-majority, and a mix of multiple minority and majority classes). Metric scores were sometimes undefined, overoptimistic, or overconservative while often neglecting the performance of minority classes. Finding adequate (new) metrics for multiclass imbalance scenarios therefore remains a critical issue.

## Other Factors Undermining Metric Selection
Apart from the lack of appropriate metrics in multiclass imbalanced scenarios, metric selection is further hindered by the limited understanding of metric behavior [@Pereira2018; @Pereira2020; @Racz2019; @Sokolova2009]. This is caused by the lack of research using comprehensive methodologies [@Haixiang2017; @Hegarcia2009; @Mullick2020] including statistical analyses that provide robust comparative conclusions, and covering a broad range of imbalance levels up to extreme imbalance [@Daskalaki2006; @Hancock2023; @Krawczyk2016] especially in scenarios with multiple classes [@Krawczyk2016]. Furthermore, the verification of existing claims can be challenging due to inconsistent, often vague operationalization of imbalance. Contributing to this challenge is the ambiguity caused by the combination of diverse study objectives that lead to different preferences for metric characteristics and the fact that these objectives or preferences are rarely stated explicitly. Such ambiguity makes it harder to find and compare studies with similar goals, potentially introducing confusion when interpreting findings and compromising cross-domain knowledge transfer. Given that borrowing metrics from different fields is a common practice [@Sokolova2009], these obstacles are particularly problematic and can undermine adequate metric selection. 

Moreover, inconsistencies in the operationalization of class imbalance and labeling its severity (e.g., mild, moderate, and extreme) hinder the verification of claims about metrics behaving differently [@Loyola-gonzalez2016b; @Jeni2013; @Racz2019; @Zhuqiuming2020] and performing less adequately in severely imbalanced scenarios compared to those with moderate levels of imbalance [@Maratea2014]. While imbalanced data technically means the class distribution is uneven, its formalization [@Luque2019] and interpretation vary in practice. The level of class imbalance is frequently expressed by the imbalance ratio (IR): the ratio of the largest to the smallest class [@Feng2019]. Less than a decade ago [@Loyola-gonzalez2016a; @Loyola-gonzalez2016b], there was no consensus on the threshold for IR to separate imbalanced datasets from balanced datasets [@Fernandez2013; @Lopez2014]. The current default is a ratio of at least 1:1.5 [@Fernandez2013; @Lopez2014; @Loyola-gonzalez2016a; @Loyola-gonzalez2016b; @Tahiraalam2019]. However, inconsistencies persist [e.g., @Santos2018; @Xia2018]. Furthermore, the lack of explicit thresholds for labeling the severity of class imbalance can lead to confusion, as labels vary widely across studies. For instance, a ratio considered 'highly' imbalanced in one study might be labeled both 'moderate' and 'extremely' in another [@Alejo2013; @Feng2019; @Krawczyk2016; @Maratea2014; @Marques2013; @Peng2014]. While acknowledging that categorizing the severity of imbalance is partially domain-specific, the absence of standardized definitions or operationalization of imbalance hinders cross-domain knowledge transfer.

Another hinderance is the lack of research covering a broad range of imbalance levels, especially extreme imbalance. This research gap is critical as severe imbalance reduces the number of appropriate metrics [@Maratea2014; @Pereira2020; @Picek2018]. At the same time, it increases the limitations for alternative solutions like resampling techniques [@Gonghuang2012; @Krawczyk2016; @Zhou2010], therefore increasing the priority for adequate metrics in these scenarios. Yet, despite the prevalence of extreme imbalance in real-world applications [@Daskalaki2006; @Hegarcia2009; @Krawczyk2016; @Provost2001; @Saez2016; @Sun2007; @Tahir2019], most research focuses on ratios below 1:100 [@Krawczyk2016; @Picek2018; @Zhang2022]. If extreme scenarios were used, the focus was on binary metrics or testing classifiers and resampling methods. Research on metrics in multiclass imbalanced scenarios is almost always based on data with ratios below 1:100 [e.g., @Alejo2013; @Branco2017; @Mullick2020; @Picek2018], and most frequently below 1:15 [e.g., @Ballabio2018; @Ferri2009; @Kautz2017; @Racz2019; @Sanchez-Crisostomo2014]. As a result of this lack of diversity in studied imbalance levels, the proposed metrics are often not robust to real-world scenarios with more severe imbalance [@Branco2017; @Hancock2023; @Mullick2020]. Therefore, the demand for appropriate metrics in extreme imbalanced scenarios remains [@Pereira2020].

At last, metric selection is hindered by previously used research designs and analytical approaches that do not provide robust comparative conclusions needed for a deep understanding of metrics and their differences. The lack of comprehensive methodologies is recognized in previous literature [e.g., @Haixiang2017; @Mullick2020]. Research has focused on demonstrating metric unsuitability through highly specific (empirical) examples, exploring (dis)similarities between metrics, and by examining how metrics inconsistently ranked results produced by different classifiers or resampling methods in various scenarios. In these studies, statistical methods like sum of ranking differences [e.g., @Racz2019], correlation or clustering techniques [e.g., @Ferri2009; @Pereira2020], and critical distance analysis [e.g., @Loyola-gonzalez2016b] were typically chosen. However, in many studies [e.g., @Branco2017; @Delgado2019; @Mullick2020; @Pereira2017] statistical validation was missing. Regardless of the analytic approach, existing research designs provide limited insights into metric behavior: they show metrics differ across scenarios but lack foundation to understand the implications of these differences. The evidence merely indicates that metric selection is critical, yet it offers little practical guidance for choosing appropriate metrics. An in-depth statistical comparison of metrics in imbalanced scenarios that can address this gap is rarely conducted. One step in that direction is the study by @Kautz2017, which is especially interesting because it examines metric behavior in multiclass imbalanced scenarios including that of MCC, F1~m~, and Accuracy. While offering valuable insights, their comparative analysis was limited to direct statistical comparison only with a new metric (multiclass performance score, MPS). Also, their ANOVA-based analyses were conducted separately for each metric and in the absence of reported effect sizes. This limits the comparative conclusions that can be drawn and the understanding of the practical importance of the results. Further methodological development could provide greater insight into metric behavior, their differences, and suitability, essential for adequate metric selection in different classification problems.

## Case Study: Credit Management
The same challenges with finding, accessing, and selecting metrics, as well as the underperformance of MCC were encountered during my internship in 2019--2020 [@VanGestel2020] at a financial credit management company (CC). The classification task involved predicting when companies would pay their overdue invoices, with categories just-past-due (0--30 days), moderately late (31--60 days), very late (61--90 days), and extremely late (91+ days). With the just-past-due category as the reference class, class imbalance increased with each subsequent payment category from mild (IR = 1:3), to severe (IR = 1:30), up to extreme (IR = 1:300) imbalance, as did the importance of accurately predicting them. The model would ultimately be used to build company risk profiles. Each profile corresponds to a collection strategy, that aims to effectively reduce payment delays while protecting the relationship with the company proportionate to the assessed risk. Given the risk of insolvency for companies that fail to pay their invoices for more than 90 days after due date, the primary objective was to improve prediction of these extremely long outstanding invoices. However, this emphasis could not lead to a disproportionate increase in false positives for that same class [@ZhangGao2014], as these false alarms could lead to unnecessary time-consuming and costly investigations [@Maldonado2020; @Wei2013] or negatively affect customer relations [@Daskalaki2006]. These user preferences and the severity of class imbalance made the classification task challenging, but selecting metrics appropriate for these scenarios was even more challenging. 

In the finance domain, particularly in credit (risk) management, practitioners 'borrow' metrics from other research domains. Validation of model performance is understudied. Consequently, the understanding needed to select suitable metrics is limited, resulting in a restricted set of commonly used evaluation metrics [@Chen2016]. While cross-domain knowledge transfer is crucial in this field, it has been severely hindered by several factors. For example, the availability of domain-specific datasets is limited [@Saia2019; @Xia2017; @Xia2018], as they are frequently not shared because of competition and privacy reasons [@Saia2019; @Xia2018]. Researchers and practitioners often have to rely on standard datasets, such as those from KEEL or UCI repositories. However, these standard datasets are often not representative of the data complexities and classification challenges observed in credit management [@Sousa2016; @Xia2018]. Even more scarce are appropriate dataset and studies that include extreme scenarios. Despite the stated prevalence of severe class imbalance in practice [@Haixiang2016; @Wei2013], classification problems have often been simplified by using relatively low imbalance levels in field relevant studies [e.g., @Beque2017; @Huang2006; @Marques2013; @Saia2019; @Serrano-Cinca2016; @Sousa2016;  @Sun2018; @Xia2017; @Xia2018; @Yu2016; @ZhangGao2014]. This gap between research and practice creates challenges in the application of research findings in real-world scenarios [@Hancock2023; @Krawczyk2016; @Pereira2018; @Sousa2016]. At the end of the internship, the search for a suitable metric was still open. Subsequent research revealed a potential solution, leading to this current thesis.

## Potential Solution: Class Balanced Performance Metrics
A potential solution for the reported issues of evaluation metrics in imbalanced classification tasks was found in @Luque2019, where metrics were transformed into class-balanced metrics based on the assessed impact of imbalance on metrics in binary classification tasks (for more details, see Appendix). In short, the elements of the confusion matrix were rewritten based on their ratio with the total number of observations of the actual class. They then used an imbalance coefficient ($\delta$) to rewrite the ratio between positives and negatives ($\frac{N}{P}$) as $\frac{1-\delta}{1+\delta}$ and reformulate the metric functions accordingly. By setting $\delta$ to zero and simplifying the equations, class-balanced metrics were created free from class imbalance related bias. While offering a promising solution, the approach was limited to binary scenarios and to the binary formulation of MCC. 

## This Thesis
This thesis aims to address this gap through the development of MCC~cb~, the multiclass version of MCC adjusted in a class-balanced manner (see Appendix). MCC was chosen for its broad applicability and promising performance in both balanced and imbalanced scenarios, while its limitations could potentially be addressed by the adjustments in MCC~cb~.

In order to establish the adequacy of MCC~cb~, its behavior is compared with the commonly used metrics Accuracy, F1~m~, and MCC. The latter three are selected because they all evaluate the confusion matrix, which is the most widely used approach in classification evaluation [@Li2019b], and given their popularity in this category. Another reason is that the development of MCC~cb~ was motivated by the rejection of F1~m~ and MCC in the selection process in the case study because of their inconsistent performance in multiclass imbalanced scenarios. This makes F1~m~ and MCC, along with Accuracy, logical choices to compare MCC~cb~ with. In contrast, MPS is not considered as it can be computationally intensive, it has only been tested on relatively low levels of imbalance, and it relies on resampling methods [@Kautz2017] which are found to have limitations with severe imbalanced data distributions [@Krawczyk2016; @Zhou2010]. 

Metric behavior is studied by simulating classification results in the form of confusion matrices, with patterns that that mimic specific classification errors (e.g., complete misclassification of the minority class) given various data structures (e.g., class imbalance and class composition) that challenges the adequacy of metrics in multiclass scenarios. The scenarios in this thesis cover the challenges observed in the case study: a four-class problem, data with multi-minority or mixed class compositions, class distributions ranging from low to extreme levels of imbalance, and the preference for a metric that optimizes the representation of minority class(es) performance without severely compromising that of the majority class(es). The aim of thesis is to establish MCC~cb~'s adequacy, while increasing understanding of the behavior and suitability of all four metrics. Therefore, the main research question of this thesis is: Is MCC~cb~ an improvement over the other commonly used metrics in imbalanced scenarios? It was hypothesized that MCC~cb~ would be more appropriate for multiclass imbalanced data than the other metrics given its design as a class-balanced metric. A key part of this thesis is the statistical comparative analysis along with reported effect sizes, which allows for more comprehensive comparative and meaningful conclusions [@Garcia2018]. The insights of this research may provide a deeper understanding of metric behavior in multiclass (im)balanced scenarios to aid the metric selection process, and establish MCC~cb~'s adequacy to expand the number of appropriate metrics available.
