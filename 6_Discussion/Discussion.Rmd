# Discussion
## Summary of the Results
This thesis studied the behavior of Accuracy, F1~m~, MCC, and the newly proposed metric MCC~cb~, establishing MCC~cb~'s adequacy for the credit management case study while providing greater understanding of the behavior of existing metrics to aid in the metric selection process for multiclass imbalanced classification tasks. The results showed that MCC~cb~ was unaffected by both class imbalance and class composition of minority and majority classes (A-D), consistently identified random classification (A), was sensitive to minority class misclassifications (B), and remained reliable with majority class misclassifications (C). Therefore, MCC~cb~ proved to be a suitable evaluation metric for the multiclass imbalanced classification scenarios simulated in this research. Furthermore, throughout study A-D the impact of class imbalance and the class composition was significant and varied between metrics. While MCC~cb~ was unaffected by both factors, Accuracy was the most affected (A-C), MCC was the second most affected (B-D), and F1~m~ was only slightly affected by imbalance (A-D) and minimally affected by class composition (A). Accuracy and MCC were found to be unsuitable, with potentially misleading scores, when class imbalance was severe because in some cases: it was not perceptible from their scores that the minority class was largely misclassified (B); Accuracy scores failed to signal random classification (A); Accuracy scores failed to reflect the perfectly classified minority class(es), due to its sensitivity to majority misclassifications (C); and MCC showed counter-intuitive scoring patterns with higher scores for complete than for partial misclassification of the majority class (C). This research did not find evidence that F1~m~ was unsuitable in imbalanced scenarios. Despite the fact that F1~m~ did not strictly meet the criteria of @Boughorbel2017 in random classifications scenarios, its behavior was not considered misleading as its values consistently reflected that classes were poorly classified (A). Similar to MCC~cb~, F1~m~ was sensitive to minority misclassifications (B) and remained reliable with majority misclassifications (C). However, F1~m~ and MCC~cb~ showed distinct preferences for the allocation of misclassifications (D). Throughout all levels of imbalance, MCC~cb~ consistently preferred allocating the misclassified instances to classes with perfect recall, whereas allocation to classes with compromised recall received the lowest scores. In contrast, F1~m~ preferences were affected by class imbalance. In imbalanced scenarios, F1~m~ preferred majority classes, whereas minority classes with perfect recall received the lowest scores. In balanced scenarios, the preferences of F1~m~ could not necessarily be linked to specific properties of the falsely assigned class but rather to the overall allocation pattern. Specifically, F1~m~ preferred patterns where the negative impact of classification errors (affecting precision and recall) was concentrated rather than spread across classes. This led to conflicting results with MCC~cb~ in cases where misclassifications were distributed over classes with perfect recall (affecting four classes) or when two classes were misclassified as each other (affecting two classes). In conclusion, both F1~m~ and MCC~cb~ could be considered appropriate evaluation metrics for multiclass imbalanced classification tasks, depending on which best accommodates the user's needs and preferences.

## Comparison with Existing Literature
The findings of this thesis are largely in agreement with the existing literature. The behavior of MCC~cb~ in imbalanced scenarios was consistent with the behavior of binary class-balanced metrics in @Luque2019. The findings also supported @Branco2017, who stated that prevalence adjusted metrics, similar to the formalisation of MCC~cb~, are better at representing the minority class performance than standard metrics. In contrast, this thesis did not find evidence for the negative claims about F1~m~ in imbalanced scenarios described in previous research. Also, for Accuracy and MCC there are some findings that contradict with existing literature and suggest that nuances should be added tot the claims about their suitability in imbalanced scenarios. 

For example, Accuracy's problematic behavior of providing deceivingly high scores, only showed in multiclass scenarios with severe class imbalance combined with multi-minority class compositions (IR $\ge$ 1:30 and nMIN = 3). Problematic behavior of Accuracy was less convincing in other scenarios. This might explain the conflict with @Kautz2017, where Accuracy was found to be less influenced by class imbalance than F1~m~ and MCC. It is possible that @Kautz2017 mainly studied metric behavior with data including fewer minority classes, but details are unavailable. 

Furthermore, the positive traits of MCC, observed in binary settings, were not found in the severely imbalanced multiclass scenarios simulated in this research. Advocates of MCC might have incorrectly assumed that these positive traits hold in multiclass scenarios. In fact, MCC suffers from the same shortcomings as Accuracy in multiclass imbalanced scenarios [@Valverde-Albacete2014]. Though, MCC's ability to point out minority class misclassifications appeared to improve with a greater proportion of minority classes, specifically in scenarios with multi-minority class compositions (nMIN = 3). The conflict with the findings in this thesis and the positive claims about MCC in multiclass studies [@Delgado2019; @Jurman2012] can be explained by their use of specific, unrealistic classification outcomes probing for the lower bounds of the metrics, that would likely always be rejected in practice [@Ballabio2018].

With respect to F1~m~, this research did not find evidence for the negative claims about its behavior in imbalanced scenarios. The results of study A and B suggested that some of the negative claims on F1~m~ likely stemmed from incorrectly generalizing characteristics of F1 to multiclass scenarios. In fact, the problematic behavior related to class swapping [@Chicco2020; @Jeni2013; @Luque2019], unsymmetrical bias to imbalance [@Jeni2013; @Lipton2014; @Luque2019], and not incorporating the true negatives in its value [@Chicco2017; @Maratea2014; @Powers2011; @Widera2020], does not occur in multiclass scenarios with F1~m~. In binary settings these issues are caused by F1's need to label the minority or majority class as the positive class, whereas in multiclass settings each class is the positive class once as a result of class averaging in F1~m~ [@Ferri2009; @Wang2012]. The contradictions of study C with @Branco2017, who stated that F1~m~ was inadequate when the majority class was largely misclassified, can be explained by implementation issues resulting in undefined scores. @Branco2017 resolved these issues only for the relevance-based extensions of F1~m~. The remaining differences, especially regarding the findings in study B, can be explained by the use of varying user preferences. Others seems to prefer a greater drop in F1~m~ scores when the minority class is largely misclassified [@Alejo2013; @Branco2017; @Maratea2014; @Sanchez-Crisostomo2014], suggesting they placed an even greater emphasis on minority class performance. However, these user preferences are not always clearly stated, making it difficult for the reader to assess whether the claims in existing literature apply to their specific use case. Though, it was clear that at least the preferences described in @Sanchez-Crisostomo2014 for metrics that score scenarios such as those in study B with near zero values, did not align with those of CC. Instead, CC adopted the more common objective in imbalanced learning, which aims for a broader representation of the results so that the performance on minority class(es) can be increased without severely compromising the performance on majority class(es) [@Hegarcia2009; @Wang2012; @Yang2018; @Zarinabad2017]. 

## Strengths and Limitations
The reason that the former mentioned contradictions and nuances could be provided is that this thesis studied metric behavior more extensively than described in the existing literature. Even though study A-C were largely based on previous research, this thesis included more conditions (e.g., extreme imbalance) and examined all three class compositions (multi-minority, multi-majority, and mixed) simultaneously. With this approach rather than isolating one scenario, metric behavior could be assessed more thoroughly by comparing across these conditions to pinpoint which (combination of) conditions results in misleading or adequate behavior. Despite this comprehensive approach, several important conditions and scenarios that can affect the suitability of metrics remain unexplored. These limitations are not discussed in this section but in Focus Points for Future Research. 

Furthermore, study D provides a new perspective on metric behavior assessment based on the distribution of misclassified instances. It is often not clear from previous research why metrics prefer different classification results or what about their behavior changes in imbalanced scenarios. Study D addressed this limitation. While the shift in behavior of F1~m~ in imbalanced scenarios was pointed out earlier [@Ferri2009; @Maratea2014], the findings of study D revealed that as class imbalance increases F1~m~ scores are more affected by class precision. Consequently, the allocation of misclassifications to the majority class(es) was preferred because the precision of the majority class was far more resilient given its size. While study D provided valuable insights, not all results could be linked to the preferences of CC, because the complete cost-matrix was unknown. This is a frequently encountered limitation [@Branco2017; @Daskalaki2006; @Hegarcia2009; @Kautz2017; @Krawczyk2014; @Sousa2016; @Sun2007; @Tahir2019; @Xia2018]. A follow-up consultation with CC would be needed to examine whether it is possible to formulate more specific preferences for a more comprehensive assessment of the findings in study D.

Another valuable aspect was systematically testing metric behavior to provide statistical evidence for statements made in previous studies [e.g., @Branco2017]. Unlike studies that did include statistical inference testing, in this research a direct comparison of metric behavior was conducted through the use of mixed-model ANOVAs with metric type as a within-subject variable. In addition, effect sizes were provided, which proved to be necessary because, similar to previous research [@Hossin2015; @Kautz2017; @Racz2019], all effects were highly significant. While aiming for methodological refinement, the strengths appeared to be intertwined with the limitations of this approach.

For example, statistical analysis of the interaction between metric type and misclassification type in study B and C provided limited value in addition to the existing literature. The misclassification types (partial and complete misclassification) in these studies both represented the same principle, which was poor misclassification of either a minority (B) or a majority (C) class. This explains that higher-order interactions with misclassification type did not exceed the effect size thresholds. Reconsideration of the specific misclassification types could result in a more meaningful statistical justification for claims in existing literature. Nevertheless, the findings based on the raw results presented in Figure 3-7 revealed important implications for metrics' suitability and support the findings in existing literature. Moreover, the value of a statistical comparison of metric behavior and the influence of misclassification type was clearly demonstrated in study D.

Another point of consideration is the fact that MCC and MCC~cb~ needed to be normalized to enable a direct comparison of metric behavior. While normalization is a commonly used strategy [@Ballabio2018; @Branco2017; @Luque2019], it is unclear how suitable this is for MCC and MCC~cb~ as values below zero can only be achieved in special cases that are unrealistic in practice [@Ballabio2018]. This is especially true when their values are below the point of complete misclassification, which is -0.33 in datasets with four classes. Metric comparison might have been more meaningful with an alternative normalization function, for example by using complete misclassification as a point of reference instead of their theoretical lower bound of --1. As a consequence for this thesis, the results indicating that F1~m~ was generally the most discriminating metric between misclassification types, should be interpreted with caution. Normalization of MCC and MCC~cb~ led to elevated scores and a compressed scale, therefore inherently reducing the differences between their scores for the misclassification types.

A drawback of the statistical approach is that mixed-model ANOVAs rely on fairly strict assumptions [@Maxwell2018] that are likely to be untenable in practice [@Kroes2023]. Compared to the pilot study prior to this research project, heteroscedasticity was reduced in the current methodology by fixing *n* at 500,000. However, it was not completely removed. Also, metric scores were bounded between 0 and 1, which makes the use of ANOVA-type methods theoretically not ideal. Despite these limitations, the use of mixed-model ANOVAs were determined to be the most feasible approach and it provided analytical consistency across all simulation studies. Moreover, the omnibus test itself is considered relatively robust against heteroscedasticity with a large sample size and a balanced design [@Maxwell2018], which was the case in this thesis. The $\eta_g^2$ values should not be used for power analysis due to the assumption constraints, but for this thesis they provided valuable insights into the relative impact of the factors on metric scores. 

Moreover, the choice for $\eta_g^2$ as an effect size measure was essential because a distinction could be made between intrinsic and extrinsic factors. By treating metric type as an intrinsic or observed factor, the variance of metric type was retained in the denominator [@Olejnik2003]. The benefits of this approach were twofold. It prevents inflated $\eta_g^2$ scores by taking into account that some of the variance is related to the metrics being inherently different rather than caused by the manipulated factors. Preliminary analyses showed that without treating metric type as an observed factor, $\eta_g^2$ values were extremely large ($\eta_g^2$ > .87), and that a more conservative approach as presented in this thesis provides more meaningful insights on the impact of class imbalance, class composition, and misclassification type on metric scores. The approach also offers a theoretical advantage in relation to the sphericity violations by accounting for non-additivity [@Kroes2023]. This is important because the Greenhouse-Geisser correction does not correct the effect sizes for sphericity violations. @Kroes2023 explained this in the context of improved calculations of $\omega^2$ for within-subject designs, but the same reasoning applies to the calculation of $\eta_g^2$  in this thesis (A. Kroes, personal communication, September 27, 2023). Therefore, the effect sizes reported in this thesis are assumed to be more reliable.

## Directions for Future Research
Despite the fact that metric behavior has been extensively studied in various scenarios, there are areas that remain unexplored. For example, future research could benefit from including scenarios with more than four classes. This would enable the examination of more allocation patterns than those investigated in study D. With the scenarios in study D, it could not be determined, for instance, whether the allocation of majority class misclassifications to the other majority class was preferred because it was perfectly recalled or simply because it was a majority class. Moreover, previous research noted that MCC is similar to specificity-based metrics [@Ballabio2018], which are known to score classification outcomes with more classes systematically higher than outcomes with comparable performance but fewer classes [@Ballabio2018; @Kautz2017; @Mullick2020]. Future research should determine whether the adequate performance of MCC~cb~ can be generalized to classification tasks with more than four classes.

Furthermore, the scenarios included relatively simple imbalance distributions. The imbalance ratio only measures the level of imbalance between the smallest and largest class and is therefore unable to capture information on the intermediate classes [@Li2020; @Zhu2018; @Zhu2020]. Therefore, class imbalance was kept constant between all minority-majority class pairs to prevent confounding effects caused by more complex distributions and thus ensure that the observed effects could be attributed to imbalance ratio.  However, in real-life scenarios, class distributions are likely to be more complex. In fact, data at CC is characterized by more complex class distributions, as the level of imbalance increases with each subsequent payment category. This limitation should be addressed in future studies, for example by including scenarios where classes can be considered both a minority and majority class simultaneously depending on the class of reference. 

Another interesting area that was not fully addressed, just touched upon by including misclassification type as a factor, is the level of discriminative power of MCC~cb~. It is known that the values of MCC tend to cluster around the middle of their range [@Zhuqiuming2020] and it is hard to achieve models with high MCC values [@Brown2018; @Zhuqiuming2020]. While for most applications the discriminative power of MCC was found sufficient [@Jurman2012], the implications for MCC~cb~ and the extent to which sufficient discriminative power is preserved after class balancing the distribution needs further investigation. 

For future studies, it would also be valuable to focus on the appropriateness of metrics in applications beyond the evaluation of the final model and in real-world environments rather than controlled simulation settings. For example: when serving as an optimizing metric, given that @Boughorbel2017 demonstrated promising outcomes with MCC in binary settings; when used inside a cross-validation process, where it would select the model with the best tuned parameters; or to periodically reassess models that are actively used. Focusing on the last example, the effectivity of evaluation metrics may be challenged when the collected data of the last period is fundamentally different from the data that the model was trained on [@Mullick2020]. In the context of the case study, it may for instance occur that, as a consequence of adequate credit management, the class with extremely late paid invoices (temporarily) disappears or that class imbalance shifts away from the relative importance of the classes in future collected datasets. The dynamic context of real-world environments that can challenge adequate model evaluation emphasizes the importance of the recommendation in @Ballabio2018 to always inspect the confusion matrix for classification tasks with imbalanced data. 

## Practical Implications and Final Points
While further research would deepen the understanding of metric behavior and their appropriateness in different scenarios, the findings of this research project already provided valuable insights and the adequacy of MCC~cb~ was successfully established. As a direct result of this research project, MCC~cb~ can be adopted as an appropriate evaluation metric in balanced and imbalanced scenarios, when the representation of the performance of underrepresented class(es) is prioritized while maintaining an adequate representation of all classes. The use of Accuracy and MCC, however, should be restricted because their behavior was not consistently reliable in multiclass imbalanced scenarios, making them unsuitable. The evidence especially advises against the use of Accuracy in severely imbalanced scenarios with multi-minority class compositions and MCC in severely imbalanced scenarios with multi-majority and mixed class compositions. In contrast, there was no convincing evidence for F1~m~ being unsuitable in multiclass imbalanced scenarios. The reported shortcomings of F1~m~ in previous research appear to be attributable to its binary equivalent and in other cases the result of claims based on different, less common user preferences. Hence, MCC~cb~ and F1~m~ are both found to be appropriate evaluation metrics for multiclass (im)balanced scenarios; the choice between them depends on user preferences given that they differ in how they value different misclassification distributions. 

In the context of the case study and based on the implications of study D, MCC~cb~ proved to be more in line with the user preferences of CC than F1~m~. The findings revealed that F1~m~ scores were most affected by a decrease in minority class precision. This initially appears to be in line with the objectives in imbalanced learning, that is, sensitivity to minority class performance. However, previous research [@Lopez2013; @Wang2012] noted that this sensitivity to minority class precision was greater than to minority class recall. Consequently, F1~m~ would prefer models that have the least amount of false positives in the minority class(es), even when this is accompanied by a decrease in recall of the minority class(es). This would be problematic for CC because it means that F1~m~ would select sub-optimal models, where misclassified minority instances would be allocated to a majority class and the companies representing these instances will remain under the radar with all the consequences that follow. Instead, for CC it is important that misclassified instances of the minority class would be allocated to any other minority class rather than to a majority class. While the collection strategy might not be optimal in this case, problematic invoices will still be identified and the companies that they represent would still be monitored. In contrast to F1~m~, MCC~cb~'s behavior did not show any conflict with the user preferences of CC.  

In conclusion, MCC~cb~ was found to be the most suitable metric for the credit management case study at CC. MCC~cb~ will therefore be recommended to CC, as an appropriate metric to evaluate and select models predicting payment behavior.  

